{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm8dLv/xOKkidzDxYhMCqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomatricardi/buildLLMapplicationFree/blob/main/Learn_LangChain_GradioLLM_FreeAI_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests to create FREE AI AGENTS"
      ],
      "metadata": {
        "id": "FFyfgHcnsur2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mXrrYRj0VW8",
        "outputId": "39dcef86-c2a7-4d30-868a-ef8281cc54e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  gradio_tools huggingface_hub langchain langchain-community langgraph google-search-results faiss-cpu tiktoken duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.rich import trange, tqdm\n",
        "from rich.markdown import Markdown\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import datetime\n",
        "from rich.console import Console\n",
        "console = Console(width=90)\n",
        "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.outputs import GenerationChunk\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from gradio_client import Client\n",
        "from gradio_client import Client\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "EdcClnV7GCBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReACT examples with Chains - Using Qwen72B\n",
        "\n",
        "For example, let’s say we want to ask our model for some up-to-date information about the upcoming\n",
        "Olympic games. To do so, we are going to build a smart LangChain agent (as described in Chapter 2)\n",
        "leveraging SerpAPIWrapperWrapper (to wrap the SerpApi to navigate the web), the AgentType tool\n",
        "(to decide which type of agent to use for our goal), and other prompt-related modules (to make it\n",
        "easier to “templatize” our instructions). Let’s see how we can do this (I won’t dive deeper into each\n",
        "component of the following code since the next chapter will be entirely focused on LangChain and\n",
        "its main components):"
      ],
      "metadata": {
        "id": "MlRvjBJmVjsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.language_models.llms import LLM\n",
        "class GradioLLMChat(LLM):\n",
        "    \"\"\"\n",
        "    Custom LLM class based on the Gradio API call.\n",
        "    \"\"\"\n",
        "    from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
        "    from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "    from langchain_core.language_models.llms import LLM\n",
        "    from langchain_core.outputs import GenerationChunk\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from gradio_client import Client\n",
        "    chatbot: Any = None\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.chatbot = Client(\"https://qwen-qwen1-5-72b-chat.hf.space/--replicas/061qr/\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Gradio API client Qwen1.5-72b_Chat\"\n",
        "\n",
        "    def _call(\n",
        "            self,\n",
        "            prompt: str,\n",
        "            stop: Optional[List[str]] = None,\n",
        "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "            chatbot=None,\n",
        "            request: float = 0.95, #it's the history touple\n",
        "            param: str = 'You are a helpful assistant', #it's the system message\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Make an API call to the Gradio API client Meta_llama3_8b using the specified prompt and return the response.\n",
        "        \"\"\"\n",
        "        if chatbot is None:\n",
        "            chatbot = self.chatbot\n",
        "\n",
        "        # Return the response from the API\n",
        "        result = chatbot.predict(   #.submit for streaming effect / .predict for normal output\n",
        "                param,\t# str  in 'Input' Textbox component\n",
        "                [],\t# Tuple[str | Dict(file: filepath, alt_text: str | None) | None, str | Dict(file: filepath, alt_text: str | None) | None]  in 'Qwen1.5-72B-Chat' Chatbot component\n",
        "                prompt,\t# str  in 'parameter_9' Textbox component\n",
        "                api_name=\"/model_chat\"\n",
        "        )\n",
        "        return result[1][0][1]"
      ],
      "metadata": {
        "id": "yWqG_xF8wCok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradioLLM + DuckDuckGo AI agent tool"
      ],
      "metadata": {
        "id": "J8OyTTJUeriT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#from https://python.langchain.com/v0.1/docs/integrations/tools/ddg/\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "#wrapper = DuckDuckGoSearchAPIWrapper(region=\"de-de\", time=\"d\", max_results=8)\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "from langchain.schema import HumanMessage\n",
        "from gradio_client import Client\n",
        "\n",
        "model = GradioLLMChat()\n",
        "search = DuckDuckGoSearchAPIWrapper(max_results=5)\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=search.run,\n",
        "        name=\"Search\",\n",
        "        description=\"useful for when you need to answer questions about current events, search online news\"\n",
        "    )\n",
        "    ]\n",
        "\n",
        "agent_executor = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur67dG6TVkk7",
        "outputId": "f7c18fd8-377e-4a5a-ea94-575518e2de50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://qwen-qwen1-5-72b-chat.hf.space/--replicas/061qr/ ✔\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(agent_executor.agent.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "SRZuqqJwVkh5",
        "outputId": "c9488b3b-3dce-4ab9-8afe-61c33c1be0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer the following questions as best you can. You have access to the following tools:\n",
              "\n",
              "\u001b[1;35mSearch\u001b[0m\u001b[1m(\u001b[0mquery: str\u001b[1m)\u001b[0m -> str - useful for when you need to answer questions about current \n",
              "events, search online news\n",
              "\n",
              "Use the following format:\n",
              "\n",
              "Question: the input question you must answer\n",
              "Thought: you should always think about what to do\n",
              "Action: the action to take, should be one of \u001b[1m[\u001b[0mSearch\u001b[1m]\u001b[0m\n",
              "Action Input: the input to the action\n",
              "Observation: the result of the action\n",
              "\u001b[33m...\u001b[0m \u001b[1m(\u001b[0mthis Thought/Action/Action Input/Observation can repeat N times\u001b[1m)\u001b[0m\n",
              "Thought: I now know the final answer\n",
              "Final Answer: the final answer to the original input question\n",
              "\n",
              "Begin!\n",
              "\n",
              "Question: \u001b[1m{\u001b[0minput\u001b[1m}\u001b[0m\n",
              "Thought:\u001b[1m{\u001b[0magent_scratchpad\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer the following questions as best you can. You have access to the following tools:\n",
              "\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Search</span><span style=\"font-weight: bold\">(</span>query: str<span style=\"font-weight: bold\">)</span> -&gt; str - useful for when you need to answer questions about current \n",
              "events, search online news\n",
              "\n",
              "Use the following format:\n",
              "\n",
              "Question: the input question you must answer\n",
              "Thought: you should always think about what to do\n",
              "Action: the action to take, should be one of <span style=\"font-weight: bold\">[</span>Search<span style=\"font-weight: bold\">]</span>\n",
              "Action Input: the input to the action\n",
              "Observation: the result of the action\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"font-weight: bold\">(</span>this Thought/Action/Action Input/Observation can repeat N times<span style=\"font-weight: bold\">)</span>\n",
              "Thought: I now know the final answer\n",
              "Final Answer: the final answer to the original input question\n",
              "\n",
              "Begin!\n",
              "\n",
              "Question: <span style=\"font-weight: bold\">{</span>input<span style=\"font-weight: bold\">}</span>\n",
              "Thought:<span style=\"font-weight: bold\">{</span>agent_scratchpad<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor('search online the news and tell me the incident on May 2024 when Giorgia Meloni met governor De Luca')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okmzCQudVken",
        "outputId": "3ba2a1d2-26b2-49e1-ef3e-596bd2bb5aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to use the search function to find relevant information about this specific event.\n",
            "Action: Search\n",
            "Action Input: \"May 2024 Giorgia Meloni governor De Luca incident\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLeggi su Sky TG24 l'articolo De Luca risponde a Meloni dopo incontro Caivano: \"Ha comunicato sua vera identità\" ... 29 mag 2024 - 14:21 ... Dal presidente del Consiglio Giorgia Meloni, ha ... \"Presidente De Luca, la stronza della Meloni!''. La premier stringe la mano al governatore della Campania così, ricordandogli come il … Nel corso di Tagadà, su La7, viene mandato il passaggio delle comunicazioni social di De Luca che a mente fredda contrattacca ad alzo zero contro Meloni. Ecco i rosicon...zi contro Giorgia. Da De ... Caivano (Napoli), 28 maggio 2024 - È ancora lite tra la premier Giorgia Meloni e Vincenzo De Luca, presidente della Regione Campania. L'occasione, questa volta, la offre l'inaugurazione del ... Premier Giorgia Meloni introduced herself as \"that bitch\" to Campania Governor Vincenzo De Luca on Tuesday after he called her a 'stronza' (bitch) when talking to reporters in the Lower House in ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: In May 2024, an incident occurred between Italian Prime Minister Giorgia Meloni and Campania Governor Vincenzo De Luca during an inauguration event in Caivano, Naples. Governor De Luca had previously referred to Meloni as a \"stronza\" (bitch) in a public statement, which led to a tense interaction between the two when they met. During their encounter, Meloni reportedly introduced herself to De Luca as \"that bitch,\" referencing his previous comment. This exchange sparked further controversy and media attention.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'search online the news and tell me the incident on May 2024 when Giorgia Meloni met governor De Luca',\n",
              " 'output': 'In May 2024, an incident occurred between Italian Prime Minister Giorgia Meloni and Campania Governor Vincenzo De Luca during an inauguration event in Caivano, Naples. Governor De Luca had previously referred to Meloni as a \"stronza\" (bitch) in a public statement, which led to a tense interaction between the two when they met. During their encounter, Meloni reportedly introduced herself to De Luca as \"that bitch,\" referencing his previous comment. This exchange sparked further controversy and media attention.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ks8llg6ajn8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}